{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1935896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from sklearn.inspection import PartialDependenceDisplay, partial_dependence\n",
    "from sklearn.metrics import (ConfusionMatrixDisplay, accuracy_score, precision_recall_curve, roc_auc_score, roc_curve)\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import shap\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdd0aa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook import notebookapp\n",
    "import urllib\n",
    "import json\n",
    "import ipykernel\n",
    "from shutil import copy2\n",
    "\n",
    "def notebook_path():\n",
    "    \"\"\"Returns the absolute path of the Notebook or None if it cannot be determined\n",
    "    NOTE: works only when the security is token-based or there is also no password\n",
    "    \"\"\"\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[1].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  # No token and no password, ahem...\n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions?token='+srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            for sess in sessions:\n",
    "                if sess['kernel']['id'] == kernel_id:\n",
    "                    return os.path.join(srv['notebook_dir'],sess['notebook']['path'])\n",
    "        except:\n",
    "            pass  # There may be stale entries in the runtime directory \n",
    "    return None\n",
    "\n",
    "\n",
    "def copy_current_nb(new_name):\n",
    "    nb = notebook_path()\n",
    "    if nb:\n",
    "        new_path = os.path.join(os.path.dirname(nb), new_name+'.ipynb')\n",
    "        copy2(nb, new_path)\n",
    "    else:\n",
    "        print(\"Current notebook path cannot be determined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a45f6157",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/cover_type_engineered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4da8f093",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, [col for col in df if not col.startswith('Cover_Type_')]]\n",
    "\n",
    "X = df.drop(columns=['Cover_Type', 'Aspect_Sector', 'rolling_mean_elevation'])\n",
    "y_train = df['Cover_Type'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4404294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccda57e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.loc[:, [col for col in X if not col.startswith('Soil_Type')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894916e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"X = X[['Wilderness_Area3',\n",
    " 'Hillshade_Noon',\n",
    " 'elevation_bin',\n",
    " 'Euclidian_Distance',\n",
    " 'Horizontal_Distance_To_Roadways',\n",
    " 'Wilderness_Area4',\n",
    " 'Vertical_Distance_To_Hydrology',\n",
    " 'Elevation',\n",
    " 'sqrt_elevation',\n",
    " 'subalpine',\n",
    " 'Wilderness_Area2',\n",
    " 'Aspect_Sector_NW',\n",
    " 'Diff_Noon_3pm',\n",
    " 'igneous_and_metamorphic',\n",
    " 'montane',\n",
    " 'alpine',\n",
    " 'Wilderness_Area1',\n",
    " 'log_elevation',\n",
    " 'reciprocal_elevation',\n",
    " 'Horizontal_Distance_To_Fire_Points',\n",
    " 'elevation_squared',\n",
    " 'montane_and_subalpine',\n",
    " 'elevation_cubed',\n",
    " 'glacial',\n",
    " 'lower_montane',\n",
    " 'Id',\n",
    " 'Hillshade_9am',\n",
    " 'Horizontal_Distance_To_Hydrology']]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8745c6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdece557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pandas.api.types\")\n",
    "\n",
    "# Assuming X and y are defined\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=.33, random_state=42)\n",
    "\n",
    "# Define the estimator\n",
    "estimator = XGBClassifier(tree_method=\"hist\")\n",
    "\n",
    "# Define hyperparameters for tuning\n",
    "hyperparameters = {\n",
    "    \"n_estimators\": stats.randint(50, 150),  # Increased range\n",
    "    \"learning_rate\": stats.uniform(0.05, 0.1),  # Adjusted range\n",
    "    \"colsample_bytree\": stats.uniform(0.6, 0.4),\n",
    "    \"colsample_bylevel\": stats.uniform(0.6, 0.4),\n",
    "    \"colsample_bynode\": stats.uniform(0.6, 0.4),\n",
    "    \"max_depth\": stats.randint(15, 50),  # Increased range\n",
    "    \"subsample\": stats.uniform(0.6, 0.4),\n",
    "    \"gamma\": stats.uniform(0, 2),  # Adjusted range\n",
    "    \"reg_lambda\": stats.uniform(0, 2),  # Adjusted range\n",
    "    \"reg_alpha\": stats.uniform(0, 2),  # Adjusted range\n",
    "}\n",
    "\n",
    "# Define the RandomizedSearchCV parameters\n",
    "random_search = RandomizedSearchCV(estimator, \n",
    "                                   param_distributions=hyperparameters, \n",
    "                                   scoring='accuracy',\n",
    "                                   return_train_score=True,\n",
    "                                   n_iter=200, \n",
    "                                   cv=5, \n",
    "                                   verbose=10, \n",
    "                                   n_jobs=-1)\n",
    "\n",
    "# Fit the RandomizedSearchCV\n",
    "try:\n",
    "    random_search.fit(X_train, y_train)  # Assuming X_train and y_train are defined\n",
    "    print(\"Best parameters found:\", random_search.best_params_)\n",
    "    print(\"Best score found:\", random_search.best_score_)\n",
    "\n",
    "    \n",
    "    # Save results\n",
    "    results_path = f\"./tuning_results/tuning_xgb/{timestamp}\"\n",
    "    if not os.path.exists(results_path):\n",
    "        os.makedirs(results_path)\n",
    "        \n",
    "    # Saving cross-validation results\n",
    "    cv_results = pd.DataFrame(random_search.cv_results_)\n",
    "    cv_results_file = f\"{timestamp}_results.csv\"\n",
    "    cv_results.to_csv(os.path.join(results_path, cv_results_file), index=False)\n",
    "    \n",
    "    # Save .ipynb\n",
    "    copy_current_nb(os.path.join(results_path, 'Evaluation_Notebook'))\n",
    "    \n",
    "    # Save Model\n",
    "    file_name = f\"xgb_{timestamp}.pkl\"\n",
    "    pickle.dump(random_search, open(os.path.join(results_path, file_name), \"wb\"))\n",
    "        \n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during model optimization: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9672402",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b94fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = f\"./tuning_results/tuning_xgb/{timestamp}/Assets\"\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5bc788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba3dd15b",
   "metadata": {},
   "source": [
    "# CV Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe237a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44981e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.sort_values(by='rank_test_score', ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f929a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_cv = cv_results.sort_values(by='rank_test_score', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c390708",
   "metadata": {},
   "source": [
    "# Train vs Test Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a11aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))    \n",
    "\n",
    "plt.plot(sorted_cv['rank_test_score'], sorted_cv['mean_train_score'], label=\"Train Score\")\n",
    "plt.plot(sorted_cv['rank_test_score'], sorted_cv['mean_test_score'], label=\"Validation Score\")\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('Sorted Validation Rank')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Test Accuracy by Final Rank')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "filename = \"test_train_by_rank.png\"\n",
    "plt.savefig(os.path.join(results_path, filename))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6361961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot algorithm comparison\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "fig.suptitle('Test Accuracy by Rank')\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.boxplot(sorted_cv.iloc[:10, :][['split0_test_score', 'split1_test_score', 'split2_test_score',\n",
    "   'split3_test_score', 'split4_test_score']].T)\n",
    "ax.set_xticklabels(range(1, 11))\n",
    "ax.set_xlabel(\"Rank\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "\n",
    "filename = \"test_accuracy_by_rank.png\"\n",
    "plt.savefig(os.path.join(results_path, filename))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd18f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot algorithm comparison\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "fig.suptitle('Train Accuracy by Rank')\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.boxplot(sorted_cv.iloc[:10, :][['split0_train_score', 'split1_train_score', 'split2_train_score',\n",
    "   'split3_train_score', 'split4_train_score']].T)\n",
    "ax.set_xticklabels(range(1, 11))\n",
    "ax.set_xlabel(\"Rank\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "\n",
    "filename = \"train_accuracy_by_rank.png\"\n",
    "plt.savefig(os.path.join(results_path, filename))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a10be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_params = cv_results.loc[cv_results['rank_test_score'] == 1]\n",
    "best_params = max_params.params.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba27bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5774cac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Train Accuracy = {max_params['mean_train_score'].values[0]:.2f}\")\n",
    "print(f\"Mean Validation Accuracy = {max_params['mean_test_score'].values[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a0f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = pickle.load(open(os.path.join(f'./tuning_results/tuning_xgb/{timestamp}/', file_name), \"rb\"))\n",
    "model = random_search.best_estimator_\n",
    "\n",
    "#model = XGBClassifier(**best_params)\n",
    "#model.fit(X_train, y_train)\n",
    "\n",
    "y_train_prediction = model.predict(X_train)\n",
    "# y_test_prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6466b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train set, Accuracy = {accuracy_score(y_train, y_train_prediction):.2f}\")\n",
    "# print(f\"Test set, Accuracy = {accuracy_score(y_test, y_test_prediction):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff53a314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping feature names to their importance scores\n",
    "feature_importance_dict = {feature_name: importance_score for feature_name, importance_score in zip(X.columns, feature_importance)}\n",
    "\n",
    "# Sort the dictionary by importance scores in descending order\n",
    "sorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Retrieve the top 10 features\n",
    "top_features = sorted_feature_importance[:15]\n",
    "\n",
    "features = [i[0] for i in top_features]\n",
    "importance = [i[1] for i in top_features]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(len(top_features)), importance, align='center')\n",
    "plt.yticks(range(len(top_features)), features)\n",
    "plt.title('Feature Importance XGB')\n",
    "plt.grid()\n",
    "\n",
    "filename = \"feature_importance.png\"\n",
    "plt.savefig(os.path.join(results_path, filename))\n",
    "            \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd43d22",
   "metadata": {},
   "source": [
    "In the context of XGBoost, these scores are often computed based on the following:\n",
    "\n",
    "- Weight: The number of times a feature appears in a tree across the ensemble of trees.\n",
    "- Gain: The average gain of a feature when it is used in trees.\n",
    "- Cover: The average coverage of a feature when it is used in trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95eb68e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110c0e7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "for col in ['param_colsample_bylevel', 'param_colsample_bynode',\n",
    "       'param_colsample_bytree', 'param_gamma', 'param_learning_rate',\n",
    "       'param_max_depth', 'param_n_estimators', 'param_reg_alpha',\n",
    "       'param_reg_lambda', 'param_subsample']:\n",
    "    \n",
    "    plt.figure(figsize=(16, 6))    \n",
    "\n",
    "    m, b = np.polyfit(list(sorted_cv['mean_test_score'].values), list(sorted_cv[col].values), 1)\n",
    "    plt.plot(sorted_cv['mean_test_score'], m * sorted_cv['mean_test_score'] + b, c='r', label=\"Regression Line\")\n",
    "    plt.scatter(sorted_cv['mean_test_score'], sorted_cv[col], label=f\"{col} Values\")\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.xlabel('Mean Validation Score')\n",
    "    plt.ylabel('Parameter Value')\n",
    "    plt.title(col)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    \n",
    "    filename = f\"{col}_by_rank.png\"\n",
    "    plt.savefig(os.path.join(results_path, filename))\n",
    "                  \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf6bb84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd071a14",
   "metadata": {},
   "source": [
    "# Hyperparameter Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c25405",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_parameters(x_values, title):\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(16, 6))\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    ax1.scatter(x_values, cv_results['mean_test_score'], label='mean_test_score', c='b')\n",
    "    #ax2.scatter(x_values, cv_results['std_test_score'], label='std_test_score', c='r')\n",
    "\n",
    "    m, b = np.polyfit(list(x_values.values), list(cv_results['mean_test_score'].values), 1)\n",
    "    ax1.plot(x_values, m * x_values + b, c='b')\n",
    "\n",
    "    m, b = np.polyfit(list(x_values.values), list(cv_results['std_test_score'].values), 1)\n",
    "    ax2.plot(x_values, m * x_values + b, c='r', label='std_test_score')\n",
    "    \n",
    "    ax1.set_title(title)\n",
    "    ax1.set_xlabel('Parameter Value')\n",
    "    ax1.set_ylabel('Mean Test Score')\n",
    "    ax2.set_ylabel('Standard Deviation of Test Score')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    \n",
    "    # Combine the legends from both axes\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc='upper right')\n",
    "\n",
    "    filename = f\"{title}_test_score.png\"\n",
    "    plt.savefig(os.path.join(results_path, filename))\n",
    "            \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01348086",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in ['param_colsample_bylevel', 'param_colsample_bynode', 'param_colsample_bytree',\n",
    "                     'param_gamma', 'param_learning_rate', 'param_max_depth', 'param_n_estimators', \n",
    "                     'param_reg_alpha', 'param_reg_lambda', 'param_subsample']:\n",
    "    x_values = cv_results[param]\n",
    "    plot_parameters(x_values, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b51b7e2",
   "metadata": {},
   "source": [
    "# Plotting Evaluation Metrics (Precision, Recall, F1-Score, AUC-ROC):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999e59eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For multiclass classification, you need to binarize the labels\n",
    "y_true_bin = label_binarize(y_train, classes=np.unique(y_train))\n",
    "y_score_bin = label_binarize(y_train_prediction, classes=np.unique(y_train_prediction))\n",
    "\n",
    "auc_roc = roc_auc_score(y_true_bin, y_score_bin, average='macro')\n",
    "\n",
    "classes = len(y_train.unique())\n",
    "\n",
    "# Plot Precision-Recall curve for each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "\n",
    "plt.figure(figsize=(16, 6))    \n",
    "for i in range(classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_true_bin[:, i], y_score_bin[:, i])\n",
    "    plt.plot(recall[i], precision[i], label='Covertype {}'.format(i + 1))\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('True Positive Rate / Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "filename = \"precision_recall.png\"\n",
    "plt.savefig(os.path.join(results_path, filename))\n",
    "            \n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot AUC-ROC curve for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "\n",
    "plt.figure(figsize=(16, 6))    \n",
    "for i in range(classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_score_bin[:, i])\n",
    "    plt.plot(fpr[i], tpr[i], label='Covertype {}'.format(i + 1))\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate / Precision')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='best')\n",
    "            \n",
    "filename = \"roc_curve.png\"\n",
    "plt.savefig(os.path.join(results_path, filename))\n",
    "            \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cd70f3",
   "metadata": {},
   "source": [
    "# Partial Dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5658a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# potentially iterate over features (and relation ie 0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3b043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features, feature_names = [(0,)], [f\"Features #{i}\" for i in range(X.shape[1])]\n",
    "deciles = {0: np.linspace(0, 1, num=5)}\n",
    "\n",
    "pd_results = partial_dependence(\n",
    "    model, X, features=1, kind=\"average\", grid_resolution=5)\n",
    "\n",
    "display = PartialDependenceDisplay(\n",
    "    [pd_results], features=features, feature_names=feature_names,\n",
    "    target_idx=0, deciles=deciles\n",
    ")\n",
    "display.plot(pdp_lim={1: (-1.38, 0.66)})\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('Feature Value')\n",
    "plt.ylabel('Partial Dependence') \n",
    "plt.title('Partial Dependence')\n",
    "\n",
    "filename = \"partial_dependence.png\"\n",
    "plt.savefig(os.path.join(results_path, filename))\n",
    "            \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e90bae3",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81de18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_names = np.unique(y_train)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [\n",
    "    (\"Confusion matrix without normalization\", None),\n",
    "    (\"Normalized confusion matrix\", \"true\"),\n",
    "]\n",
    "for title, normalize in titles_options:\n",
    "    disp = ConfusionMatrixDisplay.from_estimator(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        display_labels=class_names + 1,\n",
    "        cmap=plt.cm.Blues,\n",
    "        normalize=normalize,\n",
    "    )\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    png_name = title.lower().replace(\" \", \"_\")\n",
    "    filename = f\"{png_name}.png\"\n",
    "    plt.savefig(os.path.join(results_path, filename))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a503683c",
   "metadata": {},
   "source": [
    "# Shap Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf841a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)\n",
    "explanation = explainer.shap_values(X_train, check_additivity=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33ea864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names from X_train\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create SHAP summary plot\n",
    "shap.summary_plot(explanation, X_train, feature_names=feature_names, plot_type=\"bar\", show=False)\n",
    "\n",
    "filename = f\"shap_summary.png\"\n",
    "plt.savefig(os.path.join(results_path, filename))\n",
    "plt.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e167a74a",
   "metadata": {},
   "source": [
    "SHAP values show how each feature affects each final prediction, the significance of each feature compared to others, and the model's reliance on the interaction between features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518ff2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF WE SCALE: SCALE ONLY TRAIN DATA SET\n",
    "# BOOSTING ALG:\n",
    "\n",
    "# - Feature Selection: AUTOMATIC (NON CRITICAL)\n",
    "# - Scaling: AUTOMATIC\n",
    "# - Handling Missing Values: AUTOMATIC\n",
    "# - Outliers: NEED TO DEAL WITH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0585e12",
   "metadata": {},
   "source": [
    "# XGB Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce4d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2155de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import sort\n",
    "thresholds = sort(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d8195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model_select = []\n",
    "for thresh in thresholds:\n",
    "    # select features using threshold\n",
    "    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "    select_X_train = selection.transform(X_train)\n",
    "    \n",
    "    # train model\n",
    "    selection_model = XGBClassifier(**best_params)\n",
    "    selection_model.fit(select_X_train, y_train)\n",
    "\n",
    "    scores = cross_val_score(selection_model, select_X_train, y_train, cv=5)\n",
    "    accuracy = np.mean(scores)\n",
    "\n",
    "    print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))\n",
    "    model_select.append([thresh, select_X_train.shape[1], accuracy*100.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a533a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(model_select, columns = ['Threshold', 'Nr of Features', 'Accuracy (%)']) \n",
    "\n",
    "import dataframe_image as dfi\n",
    "dfi.export(df1.round(3), os.path.join(results_path, 'SelectFromModel.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733acdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(list(map(list, best_params.items())), columns = ['Parameter', 'Value'])\n",
    "dfi.export(df1.round(2), os.path.join(results_path, 'BestParams.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e41ec3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56175742",
   "metadata": {},
   "source": [
    "# KAGGLE Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1322c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processed = pd.read_csv('Data/test_engineered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ada85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processed = scaler.transform(test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4922e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a715bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_submission = test_processed[['Wilderness_Area3',\n",
    " 'Hillshade_Noon',\n",
    " 'elevation_bin',\n",
    " 'Euclidian_Distance',\n",
    " 'Horizontal_Distance_To_Roadways',\n",
    " 'Wilderness_Area4',\n",
    " 'Vertical_Distance_To_Hydrology',\n",
    " 'Elevation',\n",
    " 'sqrt_elevation',\n",
    " 'subalpine',\n",
    " 'Wilderness_Area2',\n",
    " 'Aspect_Sector_NW',\n",
    " 'Diff_Noon_3pm',\n",
    " 'igneous_and_metamorphic',\n",
    " 'montane',\n",
    " 'alpine',\n",
    " 'Wilderness_Area1',\n",
    " 'log_elevation',\n",
    " 'reciprocal_elevation',\n",
    " 'Horizontal_Distance_To_Fire_Points',\n",
    " 'elevation_squared',\n",
    " 'montane_and_subalpine',\n",
    " 'elevation_cubed',\n",
    " 'glacial',\n",
    " 'lower_montane',\n",
    " 'Id',\n",
    " 'Hillshade_9am',\n",
    " 'Horizontal_Distance_To_Hydrology']]\n",
    "\n",
    "X_submission = scaler.transform(X_submission)\n",
    "\n",
    "y_kaggle = model.predict(X_submission) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3c6ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"Data/Kaggle/full_submission.csv\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00930b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processed['Cover_Type'] = y_kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9bdeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_submission = test_processed.loc[:, ['Id', 'Cover_Type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b0473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_submission.Cover_Type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63093e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_submission.to_csv(f'Data/kaggle_submission_{timestamp}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03946e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(f'Data/kaggle_submission_{timestamp}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c5d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
