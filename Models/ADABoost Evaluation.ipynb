{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d75e5964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.inspection import PartialDependenceDisplay, partial_dependence\n",
    "from sklearn.metrics import (ConfusionMatrixDisplay, accuracy_score, precision_recall_curve, roc_auc_score, roc_curve)\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import shap\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca24b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook import notebookapp\n",
    "import urllib\n",
    "import json\n",
    "import ipykernel\n",
    "from shutil import copy2\n",
    "\n",
    "def notebook_path():\n",
    "    \"\"\"Returns the absolute path of the Notebook or None if it cannot be determined\n",
    "    NOTE: works only when the security is token-based or there is also no password\n",
    "    \"\"\"\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[1].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  # No token and no password, ahem...\n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions?token='+srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            for sess in sessions:\n",
    "                if sess['kernel']['id'] == kernel_id:\n",
    "                    return os.path.join(srv['notebook_dir'],sess['notebook']['path'])\n",
    "        except:\n",
    "            pass  # There may be stale entries in the runtime directory \n",
    "    return None\n",
    "\n",
    "\n",
    "def copy_current_nb(new_name):\n",
    "    nb = notebook_path()\n",
    "    if nb:\n",
    "        new_path = os.path.join(os.path.dirname(nb), new_name+'.ipynb')\n",
    "        copy2(nb, new_path)\n",
    "    else:\n",
    "        print(\"Current notebook path cannot be determined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a45f6157",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/cover_type_engineered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4da8f093",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, [col for col in df if not col.startswith('Cover_Type_')]]\n",
    "# df = df.loc[(df['Cover_Type'] == 1) | (df['Cover_Type'] == 2)]\n",
    "\n",
    "X_train = df.drop(columns=['Cover_Type', 'Aspect_Sector'])\n",
    "y_train = df['Cover_Type'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccda57e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.loc[:, [col for col in X_train if not col.startswith('Soil_Type')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe4c33f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StandardScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scaler \u001b[38;5;241m=\u001b[39m \u001b[43mStandardScaler\u001b[49m()\n\u001b[1;32m      2\u001b[0m X_train \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StandardScaler' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdece557",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pandas.api.types\")\n",
    "\n",
    "# Assuming X and y are defined\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "# Define the base estimator for AdaBoost with a more complex decision tree\n",
    "base_estimator = DecisionTreeClassifier()  \n",
    "\n",
    "# Define the AdaBoost classifier using the base estimator\n",
    "estimator = AdaBoostClassifier(estimator=base_estimator, algorithm='SAMME')\n",
    "\n",
    "# Define hyperparameters for tuning both AdaBoost and its base estimator\n",
    "hyperparameters = {\n",
    "    \"estimator__criterion\": [\"gini\", \"entropy\"],\n",
    "    \"estimator__splitter\": [\"best\", \"random\"],\n",
    "    \"estimator__max_features\": [None, \"sqrt\", \"log2\"],\n",
    "    \"estimator__max_depth\": [4, 5, 6, 7, 8, 9, 10, 11, 12],  \n",
    "    \"n_estimators\": stats.randint(50, 150),\n",
    "    \"learning_rate\": stats.uniform(0.01, 0.5),\n",
    "}\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator,\n",
    "                                   param_distributions=hyperparameters,\n",
    "                                   scoring='accuracy',\n",
    "                                   return_train_score=True,\n",
    "                                   n_iter=500,\n",
    "                                   cv=5,\n",
    "                                   verbose=10,\n",
    "                                   n_jobs=-1)\n",
    "\n",
    "# Fit the RandomizedSearchCV\n",
    "try:\n",
    "    random_search.fit(X_train, y_train)  # Assuming X_train and y_train are defined\n",
    "    print(\"Best parameters found:\", random_search.best_params_)\n",
    "    print(\"Best score found:\", random_search.best_score_)\n",
    "\n",
    "    \n",
    "    # Save results\n",
    "    results_path = f\"./tuning_results/tuning_ada/{timestamp}\"\n",
    "    if not os.path.exists(results_path):\n",
    "        os.makedirs(results_path)\n",
    "        \n",
    "    # Saving cross-validation results\n",
    "    cv_results = pd.DataFrame(random_search.cv_results_)\n",
    "    cv_results_file = f\"{timestamp}_results.csv\"\n",
    "    cv_results.to_csv(os.path.join(results_path, cv_results_file), index=False)\n",
    "    \n",
    "    # Save .ipynb\n",
    "    copy_current_nb(os.path.join(results_path, 'Evaluation_Notebook'))\n",
    "    \n",
    "    # Save Model\n",
    "    file_name = f\"ada_{timestamp}.pkl\"\n",
    "    pickle.dump(random_search, open(os.path.join(results_path, file_name), \"wb\"))\n",
    "    \n",
    "    # random_search = pickle.load(open(file_name, \"rb\"))\n",
    "    \n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during model optimization: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9672402",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b94fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = f\"./tuning_results/tuning_ada/{timestamp}/Assets\"\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f58910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba3dd15b",
   "metadata": {},
   "source": [
    "# CV Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f0970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44981e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.sort_values(by='rank_test_score', ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f929a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_cv = cv_results.sort_values(by='rank_test_score', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c390708",
   "metadata": {},
   "source": [
    "# Train vs Test Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a11aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))    \n",
    "\n",
    "plt.plot(sorted_cv['rank_test_score'], sorted_cv['mean_train_score'], label=\"Train Score\")\n",
    "plt.plot(sorted_cv['rank_test_score'], sorted_cv['mean_test_score'], label=\"Validation Score\")\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('Sorted Validation Rank')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Test Accuracy by Final Rank')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "filename = \"test_train_by_rank.png\"\n",
    "plt.savefig(os.path.join(results_path, filename))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e7d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot algorithm comparison\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "fig.suptitle('Test Accuracy by Rank')\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.boxplot(sorted_cv.iloc[:10, :][['split0_test_score', 'split1_test_score', 'split2_test_score',\n",
    "   'split3_test_score', 'split4_test_score']].T)\n",
    "ax.set_xticklabels(range(1, 11))\n",
    "ax.set_xlabel(\"Rank\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "\n",
    "filename = \"test_accuracy_by_rank.png\"\n",
    "plt.savefig(os.path.join(results_path, filename))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da14a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot algorithm comparison\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "fig.suptitle('Train Accuracy by Rank')\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.boxplot(sorted_cv.iloc[:10, :][['split0_train_score', 'split1_train_score', 'split2_train_score',\n",
    "   'split3_train_score', 'split4_train_score']].T)\n",
    "ax.set_xticklabels(range(1, 11))\n",
    "ax.set_xlabel(\"Rank\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "\n",
    "filename = \"train_accuracy_by_rank.png\"\n",
    "plt.savefig(os.path.join(results_path, filename))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a10be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_params = cv_results.loc[cv_results['rank_test_score'] == 1]\n",
    "best_params = max_params.params.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5774cac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Train set, Accuracy = {max_params['mean_train_score'].values[0]:.2f}\")\n",
    "print(f\"Mean Test  set, Accuracy = {max_params['mean_test_score'].values[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a0f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = pickle.load(open(os.path.join(f'./tuning_results/tuning_ada/{timestamp}/', file_name), \"rb\"))\n",
    "model = random_search.best_estimator_\n",
    "\n",
    "#model = ADAClassifier(**best_params)\n",
    "#model.fit(X_train, y_train)\n",
    "\n",
    "y_train_prediction = model.predict(X_train)\n",
    "y_test_prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6466b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train set, Accuracy = {accuracy_score(y_train, y_train_prediction):.2f}\")\n",
    "print(f\"Test set, Accuracy = {accuracy_score(y_test, y_test_prediction):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff53a314",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.argpartition(model.feature_importances_, -20)[-20:]\n",
    "\n",
    "features = X.columns[ind]\n",
    "importance = model.feature_importances_[ind]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(len(ind)), importance, align='center')\n",
    "plt.yticks(range(len(ind)), features)\n",
    "plt.title('Feature Importance ADA')\n",
    "plt.grid()\n",
    "\n",
    "filename = \"feature_importance.png\"\n",
    "plt.savefig(os.path.join(results_path, filename))\n",
    "            \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110c0e7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "for col in ['param_estimator__max_depth', 'param_n_estimators', 'param_learning_rate']:\n",
    "    \n",
    "    plt.figure(figsize=(16, 6))    \n",
    "\n",
    "    m, b = np.polyfit(list(sorted_cv['mean_test_score'].values), list(sorted_cv[col].values), 1)\n",
    "    plt.plot(sorted_cv['mean_test_score'], m * sorted_cv['mean_test_score'] + b, c='r', label=\"Regression Line\")\n",
    "    plt.scatter(sorted_cv['mean_test_score'], sorted_cv[col], label=f\"{col} Values\")\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.xlabel('Mean Validation Score')\n",
    "    plt.ylabel('Parameter Value')\n",
    "    plt.title(col)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    \n",
    "    filename = f\"{col}_by_rank.png\"\n",
    "    plt.savefig(os.path.join(results_path, filename))\n",
    "                  \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd071a14",
   "metadata": {},
   "source": [
    "# Hyperparameter Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c25405",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_parameters(x_values, title):\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(16, 6))\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    ax1.scatter(x_values, cv_results['mean_test_score'], label='mean_test_score', c='b')\n",
    "    #ax2.scatter(x_values, cv_results['std_test_score'], label='std_test_score', c='r')\n",
    "\n",
    "    m, b = np.polyfit(list(x_values.values), list(cv_results['mean_test_score'].values), 1)\n",
    "    ax1.plot(x_values, m * x_values + b, c='b')\n",
    "\n",
    "    m, b = np.polyfit(list(x_values.values), list(cv_results['std_test_score'].values), 1)\n",
    "    ax2.plot(x_values, m * x_values + b, c='r', label='std_test_score')\n",
    "    \n",
    "    ax1.set_title(title)\n",
    "    ax1.set_xlabel('Parameter Value')\n",
    "    ax1.set_ylabel('Mean Test Score')\n",
    "    ax2.set_ylabel('Standard Deviation of Test Score')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    \n",
    "    # Combine the legends from both axes\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc='upper right')\n",
    "\n",
    "    filename = f\"{title}_test_score.png\"\n",
    "    plt.savefig(os.path.join(results_path, filename))\n",
    "            \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01348086",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for param in ['param_estimator__max_depth', 'param_n_estimators', 'param_learning_rate']:\n",
    "\n",
    "    x_values = cv_results[param]\n",
    "    plot_parameters(x_values, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b51b7e2",
   "metadata": {},
   "source": [
    "# Plotting Evaluation Metrics (Precision, Recall, F1-Score, AUC-ROC):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999e59eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For multiclass classification, you need to binarize the labels\n",
    "y_true_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "y_score_bin = label_binarize(y_test_prediction, classes=np.unique(y_test_prediction))\n",
    "\n",
    "auc_roc = roc_auc_score(y_true_bin, y_score_bin, average='macro')\n",
    "\n",
    "# Plot Precision-Recall curve for each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "\n",
    "plt.figure(figsize=(16, 6))    \n",
    "for i in range(7):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_true_bin[:, i], y_score_bin[:, i])\n",
    "    plt.plot(recall[i], precision[i], label='Covertype {}'.format(i + 1))\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('True Positive Rate / Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "filename = \"precision_recall.png\"\n",
    "plt.savefig(os.path.join(results_path, filename))\n",
    "            \n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot AUC-ROC curve for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "\n",
    "plt.figure(figsize=(16, 6))    \n",
    "for i in range(7):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_score_bin[:, i])\n",
    "    plt.plot(fpr[i], tpr[i], label='Covertype {}'.format(i + 1))\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate / Precision')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='best')\n",
    "            \n",
    "filename = \"roc_curve.png\"\n",
    "plt.savefig(os.path.join(results_path, filename))\n",
    "            \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cd70f3",
   "metadata": {},
   "source": [
    "# Partial Dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b169c00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# potentially iterate over features (and relation ie 0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3b043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features, feature_names = [(0,)], [f\"Features #{i}\" for i in range(X.shape[1])]\n",
    "deciles = {0: np.linspace(0, 1, num=5)}\n",
    "\n",
    "pd_results = partial_dependence(\n",
    "    model, X, features=1, kind=\"average\", grid_resolution=5)\n",
    "\n",
    "display = PartialDependenceDisplay(\n",
    "    [pd_results], features=features, feature_names=feature_names,\n",
    "    target_idx=0, deciles=deciles\n",
    ")\n",
    "display.plot(pdp_lim={1: (-1.38, 0.66)})\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('Feature Value')\n",
    "plt.ylabel('Partial Dependence') \n",
    "plt.title('Partial Dependence')\n",
    "\n",
    "filename = \"partial_dependence.png\"\n",
    "plt.savefig(os.path.join(results_path, filename))\n",
    "            \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1562efb0",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3553d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_names = np.unique(y)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [\n",
    "    (\"Confusion matrix without normalization\", None),\n",
    "    (\"Normalized confusion matrix\", \"true\"),\n",
    "]\n",
    "for title, normalize in titles_options:\n",
    "    disp = ConfusionMatrixDisplay.from_estimator(\n",
    "        model,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        display_labels=class_names + 1,\n",
    "        cmap=plt.cm.Blues,\n",
    "        normalize=normalize,\n",
    "    )\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    png_name = title.lower().replace(\" \", \"_\")\n",
    "    filename = f\"{png_name}.png\"\n",
    "    plt.savefig(os.path.join(results_path, filename))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a503683c",
   "metadata": {},
   "source": [
    "# Shap Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf841a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)\n",
    "explanation = explainer.shap_values(X_test, check_additivity=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91024c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(explanation, X_test, plot_type=\"bar\", show=False)\n",
    "\n",
    "filename = f\"shap_summary.png\"\n",
    "plt.savefig(os.path.join(results_path, filename))\n",
    "plt.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985642f4",
   "metadata": {},
   "source": [
    "SHAP values show how each feature affects each final prediction, the significance of each feature compared to others, and the model's reliance on the interaction between features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74343516",
   "metadata": {},
   "source": [
    "# KAGGLE Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processed = pd.read_csv('Data/test_engineered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d46c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9291c7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processed = test_processed.loc[:, [col for col in test_processed if not col.startswith('Cover_Type_')]]\n",
    "X_kaggle = test_processed.drop(columns=['Aspect_Sector'])\n",
    "y_kaggle = model.predict(X_kaggle) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da1f806",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"Data/Kaggle/full_submission.csv\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca4712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processed['Cover_Type'] = y_kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b4d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_submission = test_processed.loc[:, ['Id', 'Cover_Type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8b97d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_submission.Cover_Type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be93e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_submission.to_csv(f'Data/kaggle_submission_{timestamp}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0712649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(f'Data/kaggle_submission_{timestamp}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bf2a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee604cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
